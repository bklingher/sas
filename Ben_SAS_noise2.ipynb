{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bMJTqe2yfglgmR-6DySUp2cGrcH4wAkJ","timestamp":1702428881719}],"gpuType":"T4","machine_shape":"hm","authorship_tag":"ABX9TyN1Bfu4kSi5SaA4RM+IRX8/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!git clone https://github.com/sjoshi804/sas-data-efficient-contrastive-learning.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BpOJ31xUCimW","executionInfo":{"status":"ok","timestamp":1702613638904,"user_tz":360,"elapsed":5367,"user":{"displayName":"Ben Klingher","userId":"00408816520453846970"}},"outputId":"4e6fbd59-c203-4ed1-8fb9-c20197d57fcf"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'sas-data-efficient-contrastive-learning'...\n","remote: Enumerating objects: 272, done.\u001b[K\n","remote: Counting objects: 100% (52/52), done.\u001b[K\n","remote: Compressing objects: 100% (23/23), done.\u001b[K\n","remote: Total 272 (delta 36), reused 39 (delta 29), pack-reused 220\u001b[K\n","Receiving objects: 100% (272/272), 24.19 MiB | 7.56 MiB/s, done.\n","Resolving deltas: 100% (143/143), done.\n"]}]},{"cell_type":"code","source":["%cd ./sas-data-efficient-contrastive-learning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8KjuXcIXM7dk","executionInfo":{"status":"ok","timestamp":1702613638905,"user_tz":360,"elapsed":5,"user":{"displayName":"Ben Klingher","userId":"00408816520453846970"}},"outputId":"4bbf6fd0-e110-4c71-f9d6-92848e868424"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/sas-data-efficient-contrastive-learning\n"]}]},{"cell_type":"code","source":["# asposestorage==1.0.2\n","# openai-clip\n","# fast_pytorch_kmeans==0.1.6\n","# numpy==1.23.1\n","# pandas==1.5.1\n","# Pillow==9.5.0\n","# setuptools==63.4.1\n","# torch==1.12.1\n","# torchvision==0.13.1\n","# tqdm==4.64.1\n","# wandb"],"metadata":{"id":"W38Dzuv2IIfT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Remember to update the requirements.txt file before running the next line\n","\n","## Replace sas-data-efficient-contrastive-learning/requirements.txt with the block above (uncomment first)!!\n","\n","OR (individually) replace clip in requirements with openai-clip, delete Pillow==9.2.0, add wandb."],"metadata":{"id":"6iWGJTvwLyeK"}},{"cell_type":"code","source":["# Replace clip in requirements with openai-clip, delete Pillow, add wandb, or\n","# copy from the block above (uncomment first)!!\n","!pip install -r requirements.txt\n","\n","!pip install sas-pip/"],"metadata":{"id":"PgflvZk4r83R","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1702613781171,"user_tz":360,"elapsed":92669,"user":{"displayName":"Ben Klingher","userId":"00408816520453846970"}},"outputId":"871ff599-f3a2-413d-8147-8e005e9b0952"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting asposestorage==1.0.2 (from -r requirements.txt (line 1))\n","  Downloading asposestorage-1.0.2.tar.gz (3.0 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting openai-clip (from -r requirements.txt (line 2))\n","  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting fast_pytorch_kmeans==0.1.6 (from -r requirements.txt (line 3))\n","  Downloading fast_pytorch_kmeans-0.1.6.tar.gz (4.3 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting numpy==1.23.1 (from -r requirements.txt (line 4))\n","  Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pandas==1.5.1 (from -r requirements.txt (line 5))\n","  Downloading pandas-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Pillow==9.5.0 (from -r requirements.txt (line 6))\n","  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting setuptools==63.4.1 (from -r requirements.txt (line 7))\n","  Downloading setuptools-63.4.1-py3-none-any.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch==1.12.1 (from -r requirements.txt (line 8))\n","  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision==0.13.1 (from -r requirements.txt (line 9))\n","  Downloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl (19.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tqdm==4.64.1 (from -r requirements.txt (line 10))\n","  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wandb (from -r requirements.txt (line 11))\n","  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pynvml (from fast_pytorch_kmeans==0.1.6->-r requirements.txt (line 3))\n","  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.1->-r requirements.txt (line 5)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.1->-r requirements.txt (line 5)) (2023.3.post1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1->-r requirements.txt (line 8)) (4.5.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1->-r requirements.txt (line 9)) (2.31.0)\n","Collecting ftfy (from openai-clip->-r requirements.txt (line 2))\n","  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from openai-clip->-r requirements.txt (line 2)) (2023.6.3)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 11)) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 11))\n","  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 11)) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 11))\n","  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 11))\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 11)) (6.0.1)\n","Collecting setproctitle (from wandb->-r requirements.txt (line 11))\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 11)) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 11)) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 11)) (1.16.0)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 11))\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1->-r requirements.txt (line 9)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1->-r requirements.txt (line 9)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1->-r requirements.txt (line 9)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1->-r requirements.txt (line 9)) (2023.11.17)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->openai-clip->-r requirements.txt (line 2)) (0.2.12)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 11))\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Building wheels for collected packages: asposestorage, fast_pytorch_kmeans, openai-clip\n","  Building wheel for asposestorage (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for asposestorage: filename=asposestorage-1.0.2-py3-none-any.whl size=9009 sha256=4f7370633a3b418b27ff6c879f328adbab00c5b8003b4a4324de6d6cf1c2b65d\n","  Stored in directory: /root/.cache/pip/wheels/b3/9e/48/b701a2540256f797b29e07ff9cbdf06e4332f3d5eaae6fbce6\n","  Building wheel for fast_pytorch_kmeans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fast_pytorch_kmeans: filename=fast_pytorch_kmeans-0.1.6-py3-none-any.whl size=7070 sha256=52a9e2aaf69fbb153347902e88371351f702e3b3265781ae674a7e7d6c5fccc9\n","  Stored in directory: /root/.cache/pip/wheels/f4/fb/cc/7881bd0509482d0739782ae9bffce444f79903d701008b9743\n","  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=13846aae54f87a66147f90cfbc9af4fa5445bc253a53661a58c4f595a045935e\n","  Stored in directory: /root/.cache/pip/wheels/08/77/8e/8d2f862df6bf7fb4e2007062d2cbaeae49862ec7b56d041229\n","Successfully built asposestorage fast_pytorch_kmeans openai-clip\n","Installing collected packages: asposestorage, tqdm, torch, smmap, setuptools, setproctitle, sentry-sdk, pynvml, Pillow, numpy, ftfy, docker-pycreds, torchvision, pandas, openai-clip, gitdb, fast_pytorch_kmeans, GitPython, wandb\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.66.1\n","    Uninstalling tqdm-4.66.1:\n","      Successfully uninstalled tqdm-4.66.1\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.1.0+cu121\n","    Uninstalling torch-2.1.0+cu121:\n","      Successfully uninstalled torch-2.1.0+cu121\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 67.7.2\n","    Uninstalling setuptools-67.7.2:\n","      Successfully uninstalled setuptools-67.7.2\n","  Attempting uninstall: Pillow\n","    Found existing installation: Pillow 9.4.0\n","    Uninstalling Pillow-9.4.0:\n","      Successfully uninstalled Pillow-9.4.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.5\n","    Uninstalling numpy-1.23.5:\n","      Successfully uninstalled numpy-1.23.5\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.16.0+cu121\n","    Uninstalling torchvision-0.16.0+cu121:\n","      Successfully uninstalled torchvision-0.16.0+cu121\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.5.3\n","    Uninstalling pandas-1.5.3:\n","      Successfully uninstalled pandas-1.5.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","cvxpy 1.3.2 requires setuptools>65.5.1, but you have setuptools 63.4.1 which is incompatible.\n","google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.5.1 which is incompatible.\n","tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.1 which is incompatible.\n","torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\n","torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\n","torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed GitPython-3.1.40 Pillow-9.5.0 asposestorage-1.0.2 docker-pycreds-0.4.0 fast_pytorch_kmeans-0.1.6 ftfy-6.1.3 gitdb-4.0.11 numpy-1.23.1 openai-clip-1.0.1 pandas-1.5.1 pynvml-11.5.0 sentry-sdk-1.39.1 setproctitle-1.3.3 setuptools-63.4.1 smmap-5.0.1 torch-1.12.1 torchvision-0.13.1 tqdm-4.64.1 wandb-0.16.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","_distutils_hack","numpy","pkg_resources","setuptools"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing ./sas-pip\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from sas==1.0) (1.12.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sas==1.0) (0.13.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sas==1.0) (1.23.1)\n","Requirement already satisfied: fast-pytorch-kmeans in /usr/local/lib/python3.10/dist-packages (from sas==1.0) (0.1.6)\n","Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from fast-pytorch-kmeans->sas==1.0) (11.5.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->sas==1.0) (4.5.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->sas==1.0) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sas==1.0) (9.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->sas==1.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->sas==1.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->sas==1.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->sas==1.0) (2023.11.17)\n","Building wheels for collected packages: sas\n","  Building wheel for sas (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sas: filename=sas-1.0-py3-none-any.whl size=6289 sha256=65bb5483a65d0b0967326351028a27d422c1cee093a4c46612ffbfa8464e4d33\n","  Stored in directory: /root/.cache/pip/wheels/5a/80/5f/4f77ca879b91a1f5f78a11be811243d69527d564efb1eb9d6b\n","Successfully built sas\n","Installing collected packages: sas\n","Successfully installed sas-1.0\n"]}]},{"cell_type":"markdown","source":["## The next block does most of the work.\n","\n","It creates the noisy dataset and then partitions it. The following block uses the result to run SAS, then we can determine the percentage of noisy examples removed."],"metadata":{"id":"F7JxwoNWKRH5"}},{"cell_type":"code","source":["\n","import random\n","import os\n","import torch\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torchvision\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","from PIL import Image, ImageFilter\n","\n","from sas.approx_latent_classes import clip_approx\n","from sas.subset_dataset import SASSubsetDataset\n","\n","cifar100 = torchvision.datasets.CIFAR100(\n","    \"/data/cifar100/\", transform=transforms.ToTensor(), download=True)\n","device = \"cuda:0\"\n","\n","# Choose noisy indices.\n","percent_noisy = .1\n","noisy_indices = random.sample(range(1, len(cifar100)), int(percent_noisy * len(cifar100)))\n","\n","x = cifar100[0][0]\n","a, b, c = x.shape\n","\n","def add_noise_to_image(x):\n","  a, b, c = x.shape\n","  d = torch.randn(a, b, c) * .3\n","  return x + d\n","\n","noise32 = {}\n","noise224 = {}\n","\n","# TODO: Why 224?\n","for i in noisy_indices:\n","  noise32[i] = torch.randn(3, 32, 32) * .3\n","  noise224[i] = torch.randn(3, 224, 224) * .3\n","\n","# Overwrite the CIFAR dataset with my own which replaces specific, pre-chosen\n","# samples with noise.\n","class MyDataset(torchvision.datasets.CIFAR100):\n","    def __getitem__(self, index):\n","        image, label = super().__getitem__(index)\n","        # print(image.shape)\n","        if index in noisy_indices:\n","          # return torch.zeros_like(image), label\n","          if image.shape[1] == 32:\n","            return image + noise32[i], label\n","          elif image.shape[1] == 32:\n","            return image + noise224[i], label\n","          else:\n","            print('BEN')\n","            print(image.shape)\n","        else:\n","          return image, label\n","\n","# Create dataset.\n","cifar_noisy = MyDataset(\n","    \"/data/cifar100/\", transform=transforms.ToTensor(), download=True)\n","\n","rand_labeled_examples_indices = random.sample(range(len(cifar_noisy)), 500)\n","rand_labeled_examples_labels = [\n","    cifar_noisy[i][1] for i in rand_labeled_examples_indices]\n","\n","partition = clip_approx(\n","    img_trainset=cifar_noisy,\n","    labeled_example_indices=rand_labeled_examples_indices,\n","    labeled_examples_labels=rand_labeled_examples_labels,\n","    num_classes=100,\n","    device=device\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"cd3OEUQINAA_","outputId":"709b2601-ce3a-410d-fc8a-8635b674e92d","executionInfo":{"status":"error","timestamp":1702434567433,"user_tz":360,"elapsed":191647,"user":{"displayName":"Ben Klingher","userId":"00408816520453846970"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]},{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aa71f10c5e0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n","    finally:\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1442, in _shutdown_workers\n","    self._shutdown = True\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 149, in join\n","    res = self._popen.wait(timeout)\n","  File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n","    if not wait([self.sentinel], timeout):\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n","    ready = selector.select(timeout)\n","  File \"/usr/lib/python3.10/selectors.py\", line 416, in select\n","    fd_event_list = self._selector.poll(timeout)\n","KeyboardInterrupt: \n"]},{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","BEN\n","torch.Size([3, 224, 224])\n","BEN\n","BENtorch.Size([3, 224, 224])BEN\n","\n","BEN\n","\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])BEN\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","BENBENBENBENBEN\n","torch.Size([3, 224, 224])BEN\n","\n","\n","\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","\n","BENtorch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","BEN\n","torch.Size([3, 224, 224])\n","\n","BENBENBENtorch.Size([3, 224, 224])\n","\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","BENBEN\n","BEN\n","torch.Size([3, 224, 224])\n","\n","\n","BEN\n","torch.Size([3, 224, 224])BENtorch.Size([3, 224, 224])\n","\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","BENBENBEN\n","torch.Size([3, 224, 224])\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","BEN\n","BENBEN\n","\n","\n","BENtorch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","BEN\n","BEN\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])BEN\n","\n","BENtorch.Size([3, 224, 224])\n","BEN\n","BENBEN\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","BEN\n","\n","torch.Size([3, 224, 224])\n","BENtorch.Size([3, 224, 224])BEN\n","BEN\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","BEN\n","\n","torch.Size([3, 224, 224])BENBEN\n","BEN\n","BENtorch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","\n","\n","torch.Size([3, 224, 224])BENtorch.Size([3, 224, 224])\n","BEN\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","BEN\n","\n","\n","torch.Size([3, 224, 224])\n","BENBEN\n","BEN\n","BENtorch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","BENBEN\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","\n","torch.Size([3, 224, 224])\n","BEN\n","\n","BEN\n","BENtorch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])BEN\n","\n","BENtorch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])\n","\n","BEN\n","\n","BENtorch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","BEN\n","\n","torch.Size([3, 224, 224])BENtorch.Size([3, 224, 224])BEN\n","\n","BEN\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","BEN\n","\n","\n","BENtorch.Size([3, 224, 224])BENBENtorch.Size([3, 224, 224])\n","\n","BEN\n","\n","torch.Size([3, 224, 224])BENtorch.Size([3, 224, 224])\n","\n","\n","\n","torch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])\n","BEN\n","BEN\n","BENtorch.Size([3, 224, 224])\n","BENtorch.Size([3, 224, 224])\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","BEN\n","\n","\n","BENBEN\n","\n","BEN\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","\n","BENBEN\n","\n","BEN\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","BEN\n","BEN\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])BEN\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])BEN\n","\n","\n","torch.Size([3, 224, 224])BEN\n","\n","torch.Size([3, 224, 224])\n","BEN\n","BENBEN\n","torch.Size([3, 224, 224])BEN\n","\n","BENtorch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","\n","\n","torch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","BENBENBEN\n","\n","\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","BEN\n","\n","BENtorch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","BENBEN\n","\n","\n","\n","BEN\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","\n","\n","BEN\n","torch.Size([3, 224, 224])\n","BENBEN\n","BENBENBEN\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])BEN\n","\n","torch.Size([3, 224, 224])\n","\n","\n","torch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])BEN\n","\n","torch.Size([3, 224, 224])BEN\n","\n","BEN\n","torch.Size([3, 224, 224])\n","BEN\n","\n","torch.Size([3, 224, 224])BENtorch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])BENBEN\n","\n","BEN\n","torch.Size([3, 224, 224])\n","\n","\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","BENBENBEN\n","\n","torch.Size([3, 224, 224])\n","BENtorch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","BEN\n","\n","BENBENBENBEN\n","\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","BEN\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","BEN\n","BENtorch.Size([3, 224, 224])\n","\n","\n","\n","torch.Size([3, 224, 224])BEN\n","\n","torch.Size([3, 224, 224])\n","BENBEN\n","BEN\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","BENBEN\n","\n","torch.Size([3, 224, 224])BENtorch.Size([3, 224, 224])\n","\n","BEN\n","BEN\n","\n","torch.Size([3, 224, 224])\n","BENtorch.Size([3, 224, 224])\n","\n","BENBEN\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","BEN\n","\n","torch.Size([3, 224, 224])\n","\n","BEN\n","torch.Size([3, 224, 224])\n","BEN\n","BENtorch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","\n","BENBEN\n","torch.Size([3, 224, 224])\n","BENBEN\n","torch.Size([3, 224, 224])\n","BENBEN\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])\n","\n","\n","BENtorch.Size([3, 224, 224])\n","\n","\n","torch.Size([3, 224, 224])BEN\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","BENBEN\n","BENBEN\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])BEN\n","BENBENBEN\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])BEN\n","BEN\n","\n","torch.Size([3, 224, 224])BEN\n","\n","BEN\n","BENtorch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])\n","\n","BEN\n","\n","BENtorch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])BENBEN\n","\n","torch.Size([3, 224, 224])\n","BEN\n","BEN\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","\n","BENtorch.Size([3, 224, 224])\n","\n","BENtorch.Size([3, 224, 224])\n","\n","\n","BEN\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","BENBENtorch.Size([3, 224, 224])\n","BEN\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","BENBEN\n","torch.Size([3, 224, 224])BENtorch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","BENtorch.Size([3, 224, 224])\n","BEN\n","torch.Size([3, 224, 224])\n","\n","\n","\n","BEN\n","torch.Size([3, 224, 224])\n","BENBEN\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","BEN\n","BENtorch.Size([3, 224, 224])BEN\n","BEN\n","\n","torch.Size([3, 224, 224])BENtorch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","BENBEN\n","torch.Size([3, 224, 224])\n","\n","\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])BENBEN\n","\n","torch.Size([3, 224, 224])\n","BEN\n","torch.Size([3, 224, 224])\n","BEN\n","torch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])\n","\n","\n","BEN\n","torch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])\n","BEN\n","BENtorch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","\n","BENBENBEN\n","\n","\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])BEN\n","BEN\n","torch.Size([3, 224, 224])\n","BEN\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","\n","BENBEN\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","BEN\n","torch.Size([3, 224, 224])\n","BENBEN\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])BEN\n","\n","\n","\n","torch.Size([3, 224, 224])BEN\n","BENBENtorch.Size([3, 224, 224])\n","\n","\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","BENBEN\n","BENtorch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","BEN\n","torch.Size([3, 224, 224])\n","BEN\n","BENtorch.Size([3, 224, 224])\n","BEN\n","torch.Size([3, 224, 224])\n","BEN\n","torch.Size([3, 224, 224])\n","BEN\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","BEN\n","\n","\n","\n","BEN\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","BEN\n","BEN\n","BENBENBEN\n","\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","BEN\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])BEN\n","\n","torch.Size([3, 224, 224])\n","BEN\n","\n","torch.Size([3, 224, 224])BENBEN\n","\n","\n","torch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])\n","BENBEN\n","\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])BEN\n","BENtorch.Size([3, 224, 224])\n","\n","\n","BENtorch.Size([3, 224, 224])torch.Size([3, 224, 224])BENBEN\n","\n","torch.Size([3, 224, 224])BEN\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","\n","\n","\n","BENtorch.Size([3, 224, 224])BEN\n","\n","BENBENtorch.Size([3, 224, 224])\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])BEN\n","\n","\n","BEN\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","BEN\n","\n","torch.Size([3, 224, 224])\n","BEN\n","BEN\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])BEN\n","\n","BENBEN\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","BENtorch.Size([3, 224, 224])\n","BEN\n","torch.Size([3, 224, 224])\n","\n","\n","torch.Size([3, 224, 224])BENBEN\n","\n","\n","torch.Size([3, 224, 224])\n","BENBENBEN\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])BEN\n","\n","\n","\n","\n","torch.Size([3, 224, 224])\n","\n","BENtorch.Size([3, 224, 224])\n","BENtorch.Size([3, 224, 224])BEN\n","\n","\n","torch.Size([3, 224, 224])BEN\n","\n","torch.Size([3, 224, 224])BEN\n","BENtorch.Size([3, 224, 224])BEN\n","\n","\n","BENBEN\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])BEN\n","\n","\n","BEN\n","torch.Size([3, 224, 224])BEN\n","\n","\n","BENtorch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","BENBEN\n","BENtorch.Size([3, 224, 224])\n","\n","BENtorch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","\n","BEN\n","\n","torch.Size([3, 224, 224])BEN\n","BEN\n","torch.Size([3, 224, 224])\n","BEN\n","BENtorch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])BEN\n","\n","torch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])\n","\n","BEN\n","torch.Size([3, 224, 224])\n","\n","BENtorch.Size([3, 224, 224])\n","BENtorch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","\n","BENBENBEN\n","\n","torch.Size([3, 224, 224])\n","\n","BENBEN\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])BENtorch.Size([3, 224, 224])\n","\n","BENBENBEN\n","torch.Size([3, 224, 224])\n","BEN\n","BEN\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","BEN\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])BENBEN\n","\n","BENtorch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","\n","\n","BENtorch.Size([3, 224, 224])BEN\n","\n","BENBEN\n","torch.Size([3, 224, 224])\n","BEN\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","BEN\n","torch.Size([3, 224, 224])BEN\n","\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","BEN\n","\n","torch.Size([3, 224, 224])\n","BEN\n","BEN\n","torch.Size([3, 224, 224])\n","BENBENBEN\n","torch.Size([3, 224, 224])\n","BEN\n","\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])\n","BEN\n","\n","BEN\n","torch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","BENBEN\n","BEN\n","torch.Size([3, 224, 224])BENtorch.Size([3, 224, 224])\n","\n","\n","BEN\n","BENtorch.Size([3, 224, 224])\n","\n","\n","torch.Size([3, 224, 224])\n","BENBEN\n","\n","torch.Size([3, 224, 224])\n","BEN\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","BENtorch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","\n","\n","BEN\n","torch.Size([3, 224, 224])\n","\n","BENBEN\n","BENtorch.Size([3, 224, 224])\n","\n","BEN\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","BENtorch.Size([3, 224, 224])\n","BEN\n","\n","torch.Size([3, 224, 224])\n","\n","BEN\n","torch.Size([3, 224, 224])BENtorch.Size([3, 224, 224])BEN\n","\n","BEN\n","\n","torch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])\n","BENBEN\n","torch.Size([3, 224, 224])BENtorch.Size([3, 224, 224])\n","\n","\n","\n","\n","BENBEN\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])BEN\n","BEN\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","\n","\n","BENtorch.Size([3, 224, 224])\n","\n","\n","torch.Size([3, 224, 224])BENBENBEN\n","\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","BENBEN\n","\n","torch.Size([3, 224, 224])\n","BEN\n","\n","BENBENtorch.Size([3, 224, 224])\n","\n","BENtorch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","BEN\n","\n","torch.Size([3, 224, 224])\n","BENBEN\n","\n","\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","BENtorch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","BENBEN\n","\n","torch.Size([3, 224, 224])BEN\n","BEN\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","BEN\n","\n","BEN\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])BEN\n","\n","BEN\n","BENtorch.Size([3, 224, 224])torch.Size([3, 224, 224])BEN\n","torch.Size([3, 224, 224])\n","\n","\n","\n","torch.Size([3, 224, 224])\n","BENBEN\n","torch.Size([3, 224, 224])\n","BENBEN\n","\n","torch.Size([3, 224, 224])\n","BEN\n","torch.Size([3, 224, 224])\n","BENtorch.Size([3, 224, 224])BEN\n","\n","torch.Size([3, 224, 224])\n","\n","BEN\n","BEN\n","torch.Size([3, 224, 224])\n","\n","BENBENtorch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","BEN\n","torch.Size([3, 224, 224])\n","\n","BEN\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","BEN\n","BEN\n","\n","torch.Size([3, 224, 224])\n","BENBEN\n","BEN\n","\n","torch.Size([3, 224, 224])BENtorch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])BEN\n","\n","\n","BEN\n","\n","torch.Size([3, 224, 224])\n","BENBEN\n","torch.Size([3, 224, 224])BEN\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","BEN\n","BENBEN\n","BEN\n","\n","BENtorch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","BEN\n","BEN\n","\n","BENtorch.Size([3, 224, 224])\n","BEN\n","torch.Size([3, 224, 224])\n","\n","\n","torch.Size([3, 224, 224])\n","BENtorch.Size([3, 224, 224])\n","BEN\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","BENBEN\n","torch.Size([3, 224, 224])\n","BEN\n","\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","BENBENBEN\n","\n","\n","torch.Size([3, 224, 224])\n","BENtorch.Size([3, 224, 224])torch.Size([3, 224, 224])BEN\n","\n","\n","BENtorch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","\n","torch.Size([3, 224, 224])\n","BENBEN\n","\n","torch.Size([3, 224, 224])BEN\n","\n","BENtorch.Size([3, 224, 224])\n","\n","\n","BENBEN\n","\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])BEN\n","BEN\n","\n","torch.Size([3, 224, 224])BENBEN\n","BEN\n","BENtorch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","\n","\n","\n","BENtorch.Size([3, 224, 224])\n","\n","\n","torch.Size([3, 224, 224])torch.Size([3, 224, 224])\n","\n","BENBENBEN\n","\n","torch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-65ddaff9c834>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     cifar_noisy[i][1] for i in rand_labeled_examples_indices]\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m partition = clip_approx(\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mimg_trainset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcifar_noisy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mlabeled_example_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrand_labeled_examples_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sas/approx_latent_classes.py\u001b[0m in \u001b[0;36mclip_approx\u001b[0;34m(img_trainset, labeled_example_indices, labeled_examples_labels, num_classes, device, batch_size, verbose)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m ):\n\u001b[0;32m---> 21\u001b[0;31m     Z = encode_using_clip(\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mimg_trainset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_trainset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sas/approx_latent_classes.py\u001b[0m in \u001b[0;36mencode_using_clip\u001b[0;34m(img_trainset, device, batch_size, verbose)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Encoding images using CLIP\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_comparable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IterableDataset_len_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IterableDataset_len_called\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_auto_collation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_auto_collation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m                 \u001b[0;31m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m                 \u001b[0;31m# store out-of-order samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    #   (2) to avoid sending multiple `_IterableDatasetStopIteration`s.\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 137, in collate\n    # shared memory tensor to avoid an extra copy\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 137, in <genexpr>\n    # shared memory tensor to avoid an extra copy\nTypeError: object of type 'NoneType' has no len()\n"]},{"output_type":"stream","name":"stdout","text":["\n","BENtorch.Size([3, 224, 224])\n","\n","torch.Size([3, 224, 224])\n","BEN\n","BENBEN\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])\n","BENBEN\n","\n","\n","torch.Size([3, 224, 224])\n","torch.Size([3, 224, 224])"]}]},{"cell_type":"markdown","source":["## Run SAS and then see how well it did"],"metadata":{"id":"-kljFmtALrsf"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","\n","class ProxyModel(nn.Module):\n","    def __init__(self, net, critic):\n","        super().__init__()\n","        self.net = net\n","        self.critic = critic\n","    def forward(self, x):\n","        return self.critic.project(self.net(x))\n","\n","# Load the proxy model.\n","net = torch.load(\"proxy-cifar100-resnet10-399-net.pt\")\n","critic = torch.load(\"proxy-cifar100-resnet10-399-critic.pt\")\n","proxy_model = ProxyModel(net, critic)\n","\n","\n","# Here we actually run SAS.\n","subset_fraction = 0.2\n","\n","# Get Subset using SAS\n","subset_dataset = SASSubsetDataset(\n","    dataset=cifar_noisy,\n","    subset_fraction=subset_fraction,\n","    num_downstream_classes=100,\n","    device=device,\n","    proxy_model=proxy_model,\n","    approx_latent_class_partition=partition,\n","    verbose=True\n",")"],"metadata":{"id":"BbJsRFAuWIYr","executionInfo":{"status":"ok","timestamp":1702616877209,"user_tz":360,"elapsed":4215,"user":{"displayName":"Ben Klingher","userId":"00408816520453846970"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# This block calculates the success at removing the noisy examples.\n","\n","print(len(cifar_noisy))\n","print(len(subset_dataset))\n","print(len(noisy_indices))\n","\n","intersection = set(subset_dataset.subset_indices).intersection(\n","    set(noisy_indices))\n","\n","print(len(intersection))\n","\n","print('SAS subset fraction: ' + str(subset_fraction))\n","print('Fraction of examples removed: ' + str(1 - subset_fraction))\n","\n","print('Fraction of noisy examples remaining (should be close to 0):')\n","print(len(intersection) / len(noisy_indices))\n","\n","print('Fraction of noisy examples removed (should be close to 1,' +\n","      'definitely greater than 1-|subset_fraction|):')\n","print(1 - len(intersection) / len(noisy_indices))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"13u6hnq4aQ2c","executionInfo":{"status":"ok","timestamp":1702433799064,"user_tz":360,"elapsed":4,"user":{"displayName":"Ben Klingher","userId":"00408816520453846970"}},"outputId":"d5730934-312a-49e3-ba55-c3ea1a44ea30"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["50000\n","10000\n","5000\n","1085\n","SAS subset fraction: 0.2\n","Fraction of examples removed: 0.8\n","Fraction of noisy examples remaining (should be close to 0):\n","0.217\n","Fraction of noisy examples removed (should be close to 1,definitely greater than 1-|subset_fraction|):\n","0.783\n"]}]},{"cell_type":"code","source":["!python simclr.py --arch resnet10 --dataset cifar10 --checkpoint-freq 10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8rOLVJ9sbOE","executionInfo":{"status":"ok","timestamp":1702614834291,"user_tz":360,"elapsed":936111,"user":{"displayName":"Ben Klingher","userId":"00408816520453846970"}},"outputId":"7876872a-3210-4112-9396-b6f8f6b8347f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n","==> Preparing data..\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:891: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n","  warnings.warn(\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /data/cifar10/cifar-10-python.tar.gz\n","100% 170498071/170498071 [00:13<00:00, 12939749.94it/s]\n","Extracting /data/cifar10/cifar-10-python.tar.gz to /data/cifar10/\n","Files already downloaded and verified\n","Files already downloaded and verified\n","subset_size: 50000\n","==> Building model..\n","step: 0\n","Loss: 6.040 : 100% 98/98 [01:19<00:00,  1.24it/s]\n","train_loss: 6.039570939784148\n","step: 1\n","Loss: 5.851 : 100% 98/98 [01:25<00:00,  1.15it/s]\n","train_loss: 5.851474976053043\n","step: 2\n","Loss: 5.769 : 100% 98/98 [01:25<00:00,  1.14it/s]\n","train_loss: 5.768739364585098\n","step: 3\n","Loss: 5.724 : 100% 98/98 [01:25<00:00,  1.14it/s]\n","train_loss: 5.72398168700082\n","step: 4\n","Loss: 5.682 : 100% 98/98 [01:25<00:00,  1.15it/s]\n","train_loss: 5.681907556494888\n","step: 5\n","Loss: 5.663 : 100% 98/98 [01:25<00:00,  1.15it/s]\n","train_loss: 5.663086000753909\n","step: 6\n","Loss: 5.633 : 100% 98/98 [01:25<00:00,  1.14it/s]\n","train_loss: 5.632522475962737\n","step: 7\n","Loss: 5.620 : 100% 98/98 [01:26<00:00,  1.14it/s]\n","train_loss: 5.619854990316897\n","step: 8\n","Loss: 5.606 : 100% 98/98 [01:25<00:00,  1.15it/s]\n","train_loss: 5.605636562619891\n","step: 9\n","Loss: 5.592 : 100% 98/98 [01:25<00:00,  1.14it/s]\n","train_loss: 5.592017086184755\n","Encoded 97/98: 100% 98/98 [00:06<00:00, 15.57it/s]\n","\n","L2 Regularization weight: 1e-05\n","Loss: 0.758 | Train Acc: 73.684% : 100% 100/100 [00:14<00:00,  6.68it/s]\n","Loss: 0.831 | Test Acc: 70.800% : 100% 20/20 [00:01<00:00, 13.52it/s]\n","test_acc: 70.8\n","step: 10\n","Loss: 5.587 :  26% 25/98 [00:22<01:06,  1.10it/s]\n","Traceback (most recent call last):\n","  File \"/content/sas-data-efficient-contrastive-learning/simclr.py\", line 254, in <module>\n","    main(device, 1, args)\n","  File \"/content/sas-data-efficient-contrastive-learning/simclr.py\", line 161, in main\n","    train_loss = trainer.train()\n","  File \"/content/sas-data-efficient-contrastive-learning/trainer.py\", line 109, in train\n","    loss = self.un_supcon_loss(z, num_positive)\n","  File \"/content/sas-data-efficient-contrastive-learning/trainer.py\", line 80, in un_supcon_loss\n","    sim = self.critic(z)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/sas-data-efficient-contrastive-learning/projection_heads/critic.py\", line 43, in forward\n","    return self.compute_sim(z)\n","  File \"/content/sas-data-efficient-contrastive-learning/projection_heads/critic.py\", line 34, in compute_sim\n","    sim[(i,i)][..., range(d), range(d)] = float('-inf')\n","KeyboardInterrupt\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/sas-data-efficient-contrastive-learning/wandb/offline-run-20231215_041831-0jykmz5q\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20231215_041831-0jykmz5q/logs\u001b[0m\n","^C\n"]}]},{"cell_type":"code","source":["net = torch.load(\"2023-12-1504:18:53.690899-cifar10-resnet10-9-net.pt\")\n","critic = torch.load(\"2023-12-1504:18:53.690899-cifar10-resnet10-9-critic.pt\")\n","proxy_model = ProxyModel(net, critic)"],"metadata":{"id":"hcR-nIPJsh9J","executionInfo":{"status":"ok","timestamp":1702616883962,"user_tz":360,"elapsed":3,"user":{"displayName":"Ben Klingher","userId":"00408816520453846970"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import random\n","import os\n","import torch\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torchvision\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","from PIL import Image, ImageFilter\n","\n","from sas.approx_latent_classes import clip_approx\n","from sas.subset_dataset import SASSubsetDataset\n","\n","cifar10 = torchvision.datasets.CIFAR10(\n","    \"/data/cifar10/\", transform=transforms.ToTensor(), download=True)\n","\n","x1 = cifar10[0][0].to('cuda')\n","x2 = cifar10[1][0].to('cuda')\n","y = torch.stack((x1, x2))\n","print(y.shape)\n","\n","a = proxy_model(y)\n","print(a.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yc2kFTky34-U","executionInfo":{"status":"ok","timestamp":1702617263253,"user_tz":360,"elapsed":1009,"user":{"displayName":"Ben Klingher","userId":"00408816520453846970"}},"outputId":"83925962-0ce2-48bd-e29b-cb397e131ee4"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","torch.Size([2, 3, 32, 32])\n","torch.Size([2, 128])\n"]}]},{"cell_type":"code","source":["!tar -xvf CIFAR-10-C.tar\n","!ls CIFAR-10-C"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IaMPOX4f7J5y","executionInfo":{"status":"ok","timestamp":1702617796046,"user_tz":360,"elapsed":6909,"user":{"displayName":"Ben Klingher","userId":"00408816520453846970"}},"outputId":"89b5334b-7a64-4bac-c381-da5b43cdee87"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["CIFAR-10-C/\n","CIFAR-10-C/fog.npy\n","CIFAR-10-C/jpeg_compression.npy\n","CIFAR-10-C/zoom_blur.npy\n","CIFAR-10-C/speckle_noise.npy\n","CIFAR-10-C/glass_blur.npy\n","CIFAR-10-C/spatter.npy\n","CIFAR-10-C/shot_noise.npy\n","CIFAR-10-C/defocus_blur.npy\n","CIFAR-10-C/elastic_transform.npy\n","CIFAR-10-C/gaussian_blur.npy\n","CIFAR-10-C/frost.npy\n","CIFAR-10-C/saturate.npy\n","CIFAR-10-C/brightness.npy\n","CIFAR-10-C/snow.npy\n","CIFAR-10-C/gaussian_noise.npy\n","CIFAR-10-C/motion_blur.npy\n","CIFAR-10-C/contrast.npy\n","CIFAR-10-C/impulse_noise.npy\n","CIFAR-10-C/labels.npy\n","CIFAR-10-C/pixelate.npy\n","brightness.npy\t       fog.npy\t\t   glass_blur.npy\t motion_blur.npy  snow.npy\n","contrast.npy\t       frost.npy\t   impulse_noise.npy\t pixelate.npy\t  spatter.npy\n","defocus_blur.npy       gaussian_blur.npy   jpeg_compression.npy  saturate.npy\t  speckle_noise.npy\n","elastic_transform.npy  gaussian_noise.npy  labels.npy\t\t shot_noise.npy   zoom_blur.npy\n"]}]},{"cell_type":"code","source":["# Unpack CIFAR-10-C\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","DIR = 'CIFAR-10-C'\n","lb = np.load(os.path.join(DIR,'labels.npy'))\n","cifar10c = np.load('CIFAR-10-C/fog.npy')\n","print(lb)\n","\n","cifar10c.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OdyWyN4q7DOZ","executionInfo":{"status":"ok","timestamp":1702618697292,"user_tz":360,"elapsed":319,"user":{"displayName":"Ben Klingher","userId":"00408816520453846970"}},"outputId":"ec0b3a82-137f-41f0-c4bd-7f72d1dbad16"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["[3 8 8 ... 5 1 7]\n"]},{"output_type":"execute_result","data":{"text/plain":["(50000, 32, 32, 3)"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["# Copied from sas-pip/sas/approx_latent_classes.py for debugging.\n","\n","from copy import deepcopy\n","from typing import List\n","\n","import clip\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from fast_pytorch_kmeans import KMeans\n","from tqdm import tqdm\n","\n","def clip_approx(\n","    img_trainset: torch.utils.data.Dataset,\n","    labeled_example_indices: List[int],\n","    labeled_examples_labels: np.array,\n","    num_classes: int,\n","    device: torch.device,\n","    batch_size: int = 512,\n","    verbose: bool = False,\n","):\n","    Z = encode_using_clip(\n","        img_trainset=img_trainset,\n","        device=device,\n","        batch_size=batch_size,\n","        verbose=verbose\n","    )\n","    clf = train_linear_classifier(\n","        X=Z[labeled_example_indices],\n","        y=torch.tensor(labeled_examples_labels),\n","        representation_dim=len(Z[0]),\n","        num_classes=num_classes,\n","        device=device,\n","        verbose=verbose\n","    )\n","    preds = []\n","    for start_idx in range(0, len(Z), batch_size):\n","        preds.append(torch.argmax(clf(Z[start_idx:start_idx + batch_size]).detach(), dim=1).cpu())\n","    preds = torch.cat(preds).numpy()\n","\n","    return partition_from_preds(preds)\n","\n","def clip_0shot_approx(\n","    img_trainset: torch.utils.data.Dataset,\n","    class_names: List[str],\n","    device: torch.device,\n","    verbose: bool = False,\n","):\n","    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","    img_trainset = deepcopy(img_trainset)\n","    img_trainset.transform = preprocess\n","\n","    zeroshot_weights = zeroshot_classifier(\n","        class_names=class_names,\n","        device=device,\n","        verbose=verbose\n","    )\n","    logits = []\n","    loader = torch.utils.data.DataLoader(img_trainset, batch_size=32, num_workers=2)\n","    with torch.no_grad():\n","        for input in tqdm(loader, \"0-shot classification using provided text names for classes\", disable=not verbose):\n","            # predict\n","            image_features = model.encode_image(input[0].to(device=device))\n","            image_features /= image_features.norm(dim=-1, keepdim=True)\n","            logits.append(100. * image_features @ zeroshot_weights)\n","\n","    preds = []\n","    for logit in logits:\n","        preds.append(logit.topk(1, 1, True, True)[1].t()[0])\n","\n","    return partition_from_preds(preds)\n","\n","def kmeans_approx(\n","    trainset: torch.utils.data.Dataset,\n","    proxy_model: nn.Module,\n","    num_classes: int,\n","    device: torch.device,\n","    verbose: bool = False\n","):\n","    proxy_model.eval()\n","    Z = []\n","    with torch.no_grad():\n","        loader = torch.utils.data.DataLoader(trainset, batch_size=32, num_workers=2)\n","        for input in tqdm(loader, \"Encoding data using proxy model provided\", disable=not verbose):\n","            Z.append(proxy_model(input[0].to(device)))\n","    Z = torch.cat(Z, dim=0).to(\"cpu\")\n","    kmeans = KMeans(n_clusters=num_classes, mode='euclidean', verbose=int(verbose), max_iter=1000)\n","    preds = kmeans.fit_predict(Z).cpu().numpy()\n","    return partition_from_preds(preds)\n","\n","def encode_using_clip(\n","        img_trainset: torch.utils.data.Dataset,\n","        device: torch.device,\n","        batch_size=512,\n","        verbose: bool = False,\n","):\n","    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","    img_trainset = deepcopy(img_trainset)\n","    img_trainset.transform = preprocess\n","\n","    loader = torch.utils.data.DataLoader(img_trainset, batch_size=batch_size, num_workers=8)\n","    Z = []\n","    with torch.no_grad():\n","        for input in tqdm(loader, desc=\"Encoding images using CLIP\", disable=not verbose):\n","            Z.append(model.encode_image(input[0].to(device)))\n","    Z = torch.cat(Z, dim=0).to(torch.float32)\n","    return Z\n","\n","def partition_from_preds(preds):\n","    partition = {}\n","    for i, pred in enumerate(preds):\n","        if pred not in partition:\n","            partition[pred] = []\n","        partition[pred].append(i)\n","    return partition\n","\n","def train_linear_classifier(\n","    X: torch.tensor,\n","    y: torch.tensor,\n","    representation_dim: int,\n","    num_classes: int,\n","    device: torch.device,\n","    reg_weight: float = 1e-3,\n","    n_lbfgs_steps: int = 500,\n","    verbose=False,\n","):\n","    if verbose:\n","        print('\\nL2 Regularization weight: %g' % reg_weight)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    X_gpu = X.to(device)\n","    y_gpu = y.to(device)\n","\n","    # Should be reset after each epoch for a completely independent evaluation\n","    clf = nn.Linear(representation_dim, num_classes).to(device)\n","    clf_optimizer = optim.LBFGS(clf.parameters())\n","    clf.train()\n","\n","    for _ in tqdm(range(n_lbfgs_steps), desc=\"Training linear classifier using fraction of labels\", disable=not verbose):\n","        def closure():\n","            clf_optimizer.zero_grad()\n","            raw_scores = clf(X_gpu)\n","            loss = criterion(raw_scores, y_gpu)\n","            loss += reg_weight * clf.weight.pow(2).sum()\n","            loss.backward()\n","            return loss\n","        clf_optimizer.step(closure)\n","    return clf\n","\n","def zeroshot_classifier(\n","    class_names: List[str],\n","    device: torch.device,\n","    verbose: bool = False\n","):\n","    templates = [\n","        'itap of the {}.',\n","        'a bad photo of the {}',\n","        'a origami {}.',\n","        'a photo of the large {}.',\n","        'a {} in a video game.',\n","        'art of the {}.',\n","        'a photo of the small {}.',\n","    ]\n","\n","    model, _ = clip.load(\"ViT-B/32\")\n","    model = model.to(device)\n","\n","    with torch.no_grad():\n","        zeroshot_weights = []\n","        for classname in tqdm(class_names, desc=\"Creating zero shot classifier\", disable=not verbose):\n","            texts = [template.format(classname) for template in templates] #format with class\n","            texts = clip.tokenize(texts).to(device) #tokenize\n","            class_embeddings = model.encode_text(texts) #embed with text encoder\n","            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n","            class_embedding = class_embeddings.mean(dim=0)\n","            class_embedding /= class_embedding.norm()\n","            zeroshot_weights.append(class_embedding)\n","        zeroshot_weights = torch.stack(zeroshot_weights, dim=1)\n","    return zeroshot_weights"],"metadata":{"id":"0tFeKcBoLx15","executionInfo":{"status":"ok","timestamp":1702624721788,"user_tz":360,"elapsed":487,"user":{"displayName":"Ben Klingher","userId":"00408816520453846970"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["import random\n","import os\n","import torch\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torchvision\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","from PIL import Image, ImageFilter\n","import torch.nn.functional as F\n","\n","# from sas.approx_latent_classes import clip_approx\n","# from sas.subset_dataset import SASSubsetDataset\n","\n","cifar10 = torchvision.datasets.CIFAR10(\n","    \"/data/cifar10/\", transform=transforms.ToTensor(), download=True, train=False)\n","device = \"cuda:0\"\n","\n","\n","def plt2(i, i2):\n","  \"\"\"Display two images.\"\"\"\n","  fig = plt.figure(figsize=(10, 7))\n","\n","  # setting values to rows and column variables\n","  rows = 2\n","  columns = 1\n","\n","  fig.add_subplot(rows, columns, 1)\n","\n","  plt.imshow(i)\n","  fig.add_subplot(rows, columns, 2)\n","\n","  plt.imshow(i2)\n","\n","# x = cifar10[0][0]\n","# a, b, c = x.shape\n","\n","# def add_noise_to_image(x):\n","#   a, b, c = x.shape\n","#   d = torch.randn(a, b, c) * .3\n","#   return x + d\n","\n","# noise32 = {}\n","# noise224 = {}\n","\n","# # TODO: Why 224?\n","# for i in noisy_indices:\n","#   noise32[i] = torch.randn(3, 32, 32) * .3\n","#   noise224[i] = torch.randn(3, 224, 224) * .3\n","\n","first = True\n","\n","severity = 1\n","\n","noise_types = ['fog']\n","\n","# for _ in range(1):\n","for noise_type in noise_types:\n","  print(noise_type)\n","  fname = noise_type + '.npy'\n","  DIR = 'CIFAR-10-C'\n","  cifar10c = np.load(os.path.join(DIR, fname))\n","  # cifar10c = np.load('CIFAR-10-C/fog.npy')\n","\n","  for _ in range(1):\n","  # for severity in range(1,6):\n","    # Choose noisy indices.\n","    percent_noisy = .1\n","    noisy_indices = random.sample(range(1, len(cifar10)), int(percent_noisy * len(cifar10)))\n","\n","    # Overwrite the CIFAR dataset with my own which replaces specific, pre-chosen\n","    # samples with noise.\n","    class MyDataset(torchvision.datasets.CIFAR10):\n","        def __getitem__(self, index):\n","            global first\n","            image, label = super().__getitem__(index)\n","            # print(image.shape)\n","            preprocess = transforms.Compose([\n","              transforms.Resize(256),\n","              transforms.CenterCrop(224),\n","              # transforms.ToTensor(),\n","              transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","            ])\n","            if index in noisy_indices:\n","              i = (severity - 1) * 10000 + index\n","              # print('BEN')\n","              # print(i)\n","              # print(index)\n","              # print(image.shape)\n","              #  torch.nn.functionnal.interpolate.\n","              # trans1 = transforms.ToTensor()\n","              # trans1 = transforms.Compose([transforms.ToTensor(), transforms.Resize(size=(224, 224)),\n","              #                              transforms.Normalize((0.5,), (0.5,))])\n","              # trans1 = transforms.Compose([transforms.ToTensor(), transforms.Resize(size=(224, 224)),\n","              #                             transforms.Normalize((0.5,), (0.5,))])\n","\n","              preprocess2 = transforms.Compose([\n","                transforms.Resize(256),\n","                transforms.CenterCrop(224),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","              ])\n","\n","              # a = trans1(cifar10c[i])\n","              # a = preprocess(cifar10c[i])\n","              # print(a.shape)\n","              # a = a.permute(2, 0, 1)\n","              # print(a.shape)\n","\n","              # out2 = F.interpolate(a.unsqueeze(dim=0), size=(224, 224), mode='bilinear')\n","\n","              # out2 = out2.squeeze()\n","              # print(out2.shape)\n","              # print(out2)\n","\n","              # a = torch.from_numpy(cifar10c[i])\n","              a = preprocess2(Image.fromarray(cifar10c[i]))\n","              b = torch.zeros_like(a)\n","              c = b + a\n","              # print(a.shape)\n","              # a = a.permute(2, 0, 1)\n","              # a = a / 255.0\n","              # print(a.shape)\n","              if first and image.shape[1] == 224:\n","                # print('BEN')\n","                first = False\n","                # plt2(image.permute(1, 2, 0), c.permute(1, 2, 0))\n","                # print(i)\n","                # print(c)\n","                # print(c.shape)\n","                # print(image)\n","                # print(image.shape)\n","                # assert False\n","              return c, label\n","              # return image, label\n","            else:\n","              return preprocess(image), label\n","\n","    # Create dataset.\n","    cifar_noisy = MyDataset(\n","        \"/data/cifar10/\", transform=transforms.ToTensor(), download=True, train=False)\n","\n","    rand_labeled_examples_indices = random.sample(range(len(cifar_noisy)), 500)\n","    rand_labeled_examples_labels = [\n","        cifar_noisy[i][1] for i in rand_labeled_examples_indices]\n","\n","    partition = clip_approx(\n","        img_trainset=cifar_noisy,\n","        labeled_example_indices=rand_labeled_examples_indices,\n","        labeled_examples_labels=rand_labeled_examples_labels,\n","        num_classes=100,\n","        device=device\n","    )\n","\n","    subset_fraction = .2\n","\n","    # Get Subset using SAS\n","    subset_dataset = SASSubsetDataset(\n","        dataset=cifar_noisy,\n","        subset_fraction=subset_fraction,\n","        num_downstream_classes=100,\n","        device=device,\n","        proxy_model=proxy_model,\n","        approx_latent_class_partition=partition,\n","        verbose=True\n","    )\n","\n","    print(len(cifar_noisy))\n","\n","    print(len(subset_dataset))\n","    # print(subset_dataset.subset_indices)\n","\n","    print(len(noisy_indices))\n","\n","    inter = set(subset_dataset.subset_indices).intersection(set(noisy_indices))\n","\n","    print(len(inter))\n","\n","    print(noise_type)\n","    print('SAS subset fraction: ' + str(subset_fraction))\n","\n","    print(len(inter) / len(noisy_indices))\n","\n","    print('Percent of noisy examples removed (should be close to 1, definitely greater than 1-|subset_fraction|):')\n","    print(1 - len(inter) / len(noisy_indices))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"-asm8rDy4bTf","executionInfo":{"status":"error","timestamp":1702625639665,"user_tz":360,"elapsed":36507,"user":{"displayName":"Ben Klingher","userId":"00408816520453846970"}},"outputId":"31dcfeb2-3fa7-4356-bf33-7326faf2431c"},"execution_count":89,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","fog\n","Files already downloaded and verified\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-89-045e05bb364b>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m# Get Subset using SAS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     subset_dataset = SASSubsetDataset(\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcifar_noisy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0msubset_fraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset_fraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sas/subset_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, subset_fraction, num_downstream_classes, device, approx_latent_class_partition, proxy_model, augmentation_distance, num_runs, pairwise_distance_block_size, threshold, verbose)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentation_distance\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentation_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproximate_augmentation_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mclass_wise_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sas/subset_dataset.py\u001b[0m in \u001b[0;36mapproximate_augmentation_distance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;31m# Initialize augmentation distance with all 0s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0maugmentation_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_trainset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlatent_class\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mZ_partition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlatent_class\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sas/subset_dataset.py\u001b[0m in \u001b[0;36mencode_trainset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                 \u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-1effd10603e4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load the proxy model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/sas-data-efficient-contrastive-learning/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mResNet10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/sas-data-efficient-contrastive-learning/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/sas-data-efficient-contrastive-learning/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mStemSTL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStemCIFAR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    451\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 453\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    454\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 12.25 GiB (GPU 0; 14.75 GiB total capacity; 2.52 GiB already allocated; 11.26 GiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"code","source":["# Get Subset using SAS\n","subset_dataset = SASSubsetDataset(\n","    dataset=cifar_noisy,\n","    subset_fraction=subset_fraction,\n","    num_downstream_classes=100,\n","    device=device,\n","    proxy_model=proxy_model,\n","    approx_latent_class_partition=partition,\n","    verbose=True\n",")"],"metadata":{"id":"fbsFFTa568Wl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://zenodo.org/records/2535967/files/CIFAR-10-C.tar"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQHZAoqF6xQW","executionInfo":{"status":"ok","timestamp":1702617788530,"user_tz":360,"elapsed":169254,"user":{"displayName":"Ben Klingher","userId":"00408816520453846970"}},"outputId":"4b70f4db-96b8-4ab4-ef0e-54b831063ffb"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-12-15 05:20:19--  https://zenodo.org/records/2535967/files/CIFAR-10-C.tar\n","Resolving zenodo.org (zenodo.org)... 188.185.79.172, 188.184.103.159, 188.184.98.238, ...\n","Connecting to zenodo.org (zenodo.org)|188.185.79.172|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2918471680 (2.7G) [application/octet-stream]\n","Saving to: ‘CIFAR-10-C.tar’\n","\n","CIFAR-10-C.tar      100%[===================>]   2.72G  17.3MB/s    in 2m 48s  \n","\n","2023-12-15 05:23:08 (16.6 MB/s) - ‘CIFAR-10-C.tar’ saved [2918471680/2918471680]\n","\n"]}]},{"cell_type":"code","source":["!ls -l CIFAR-10-C"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hd9biBAG6xj8","executionInfo":{"status":"ok","timestamp":1702626105669,"user_tz":360,"elapsed":320,"user":{"displayName":"Ben Klingher","userId":"00408816520453846970"}},"outputId":"8b89ae9d-55b1-416b-d09e-5c56072d97c8"},"execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["total 2850132\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 brightness.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 contrast.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 defocus_blur.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 elastic_transform.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 fog.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 frost.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 gaussian_blur.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 gaussian_noise.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 glass_blur.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 impulse_noise.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 jpeg_compression.npy\n","-rw-rw-r-- 1 1189 2002     50128 Nov 25  2018 labels.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 motion_blur.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 pixelate.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 saturate.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 shot_noise.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 snow.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 spatter.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 speckle_noise.npy\n","-rw-rw-r-- 1 1189 2002 153600128 Nov 25  2018 zoom_blur.npy\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"YMAW1gI_X9Vm"},"execution_count":null,"outputs":[]}]}